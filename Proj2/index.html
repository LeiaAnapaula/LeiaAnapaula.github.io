<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>CS180 – Project 2 by Leia</title>
  <style>
    :root { --w: 320px; --gap: 18px; }
    body { font-family: system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, sans-serif; line-height: 1.45; margin: 0; color: #222; }
    header { padding: 28px 16px; border-bottom: 1px solid #eee; }
    main { max-width: 1000px; margin: 0 auto; padding: 24px 16px 64px; }
    h1, h2, h3 { margin: 0.4em 0 0.35em; }
    section { margin: 36px 0 44px; }
    .grid { display: grid; grid-template-columns: repeat(auto-fill, minmax(var(--w), 1fr)); gap: var(--gap); align-items: start; }
    figure { margin: 0; }
    figure img { width: 100%; height: auto; display: block; border-radius: 8px; }
    figure figcaption { font-size: 0.95rem; color: #555; margin-top: 6px; }
    pre { background: #0f172a; color: #e2e8f0; padding: 12px 14px; border-radius: 8px; overflow-x: auto; font-size: 0.9rem; }
    code { font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace; }
    .muted { color: #666; font-size: 0.95rem; }
    .pill { display: inline-block; background: #eef2ff; color: #3730a3; padding: 2px 8px; border-radius: 999px; font-size: 0.8rem; margin-left: 6px; }
    .hr { height: 1px; background: #eee; margin: 28px 0; }
  </style>
</head>
<body>
  <header>
    <h1>Project 2 – Fun with Filters and Frequencies <span class="pill">Leia</span></h1>
    <p class="muted">CS180/280A • Fall 2025</p>
  </header>

  <main>
    <!-- ===================== Part 1.1 ===================== -->
    <section id="part11">
      <h2>Part 1.1 — Convolutions from Scratch</h2>
      <p>
        I implemented 2D convolution two ways: a <em>4-loop</em> version and a faster
        <em>2-loop</em> version. Verified against <code>scipy.signal.convolve2d</code>.
      </p>

      <pre><code># 4-loop conv (numpy only)
def conv2d_naive_4loops(img, kernel):
    img = img.astype(np.float32)
    k = np.array(kernel, np.float32)[::-1, ::-1]  # flip
    H, W = img.shape; kh, kw = k.shape
    ph, pw = kh//2, kw//2
    p = np.pad(img, ((ph, ph), (pw, pw)), mode='constant')
    out = np.zeros((H, W), np.float32)
    for i in range(H):
        for j in range(W):
            acc = 0.0
            for u in range(kh):
                for v in range(kw):
                    acc += p[i+u, j+v] * k[u, v]
            out[i, j] = acc
    return out</code></pre>

      <pre><code># 2-loop conv (vectorized patch)
def conv2d_naive_2loops(img, kernel):
    img = img.astype(np.float32)
    k = np.array(kernel, np.float32)[::-1, ::-1]
    H, W = img.shape; kh, kw = k.shape
    ph, pw = kh//2, kw//2
    p = np.pad(img, ((ph, ph), (pw, pw)), mode='constant')
    out = np.zeros((H, W), np.float32)
    for i in range(H):
        for j in range(W):
            region = p[i:i+kh, j:j+kw]
            out[i, j] = np.sum(region * k)
    return out</code></pre>

      <div class="grid">
        <figure><img src="figs/blur9.png"><figcaption>Blurred Selfie</figcaption></figure>
        <figure><img src="figs/Gx.png"><figcaption>Partial Derivative Gx Selfie</figcaption></figure>
        <figure><img src="figs/Gy.png"><figcaption>Partial Derivative Gy Selfie</figcaption></figure>
        <figure><img src="figs/grad_mag.png"><figcaption>Gradient Magnitude Selfie</figcaption></figure>
      </div>

      <div class="hr"></div>
    </section>

    <!-- ===================== Part 1.2 ===================== -->
    <section id="part12">
      <h2>Part 1.2 — Finite Difference Operator</h2>
      <p>
        Convolved <em>cameraman</em> with \(D_x=[1,-1]\) and \(D_y=[1\; -1]^T\), then formed 
        \( \sqrt{G_x^2 + G_y^2} \) and thresholded it to make a binary edge map.
      </p>

      <pre><code># Using convolve2d to compute finite differences
gx = convolve2d(img, np.array([[1,-1]]), mode='same', boundary='symm')
gy = convolve2d(img, np.array([[1],[-1]]), mode='same', boundary='symm')
mag = np.sqrt(gx**2 + gy**2)
edges = (mag > 0.1).astype(np.float32)</code></pre>

      <div class="grid">
        <figure><img src="figs/cameraman_gx.png"><figcaption>Gx.</figcaption></figure>
        <figure><img src="figs/cameraman_gy.png"><figcaption>Gy.</figcaption></figure>
        <figure><img src="figs/cameraman_mag.png"><figcaption>Magnitude.</figcaption></figure>
        <figure><img src="figs/cameraman_edges.png"><figcaption>Edges after thresholding.</figcaption></figure>
      </div>

      <div class="hr"></div>
    </section>

    <!-- ===================== Part 1.3 ===================== -->
    <section id="part13">
      <h2>Part 1.3 — Derivative of Gaussian (DoG)</h2>
      <p>
        To reduce noise, I smooth with a Gaussian then differentiate (or use DoG kernels). 
        Edges are much cleaner than raw finite differences.
      </p>

      <pre><code># DoG filter example
g = cv2.getGaussianKernel(ksize=7, sigma=1.5)
gx = -np.arange(-3,4)/1.5**2 * g[:,0]  # derivative part
dogx = convolve2d(img, gx[np.newaxis,:], mode='same', boundary='symm')</code></pre>

      <div class="grid">
        <figure><img src="figs/cameraman_dogx.png"><figcaption>DoG \(G_x\).</figcaption></figure>
        <figure><img src="figs/cameraman_dogy.png"><figcaption>DoG \(G_y\).</figcaption></figure>
        <figure><img src="figs/grad_mag.png"><figcaption>DoG magnitude.</figcaption></figure>
      </div>

      <div class="hr"></div>
    </section>

    <!-- ===================== Part 2.1 ===================== -->
    <section id="part21">
      <h2>Part 2.1 — Image “Sharpening” (Unsharp Mask)</h2>
      <p>
        Unsharp masking: <code>sharpened = image + α × (image − blur)</code>.
      </p>
    
      <h3>Taj Mahal</h3>
      <div class="grid">
        <figure><img src="taj.jpg"><figcaption>Taj original.</figcaption></figure>
        <figure><img src="figs/taj_unsharp.png"><figcaption>Taj sharpened.</figcaption></figure>
      </div>
    
      <h3>Siblings</h3>
      <div class="grid">
        <figure><img src="siblings.jpg"><figcaption>Siblings original.</figcaption></figure>
        <figure><img src="figs/siblings_unsharp.png"><figcaption>Siblings sharpened (α=1.5, σ=1.0).</figcaption></figure>
      </div>
    
      <h3>Blur → Sharpen Experiment</h3>
      <p class="muted">
        I picked a sharp image, blurred it, then tried to sharpen it again.  
        Observations: The “recovered” image looks sharper and more detailed than the blurred version, 
        but it cannot fully restore the lost high frequencies. Edges regain contrast but fine textures 
        do not fully come back—showing unsharp masking enhances but does not recreate detail.
      </p>
      <div class="grid">
        <figure><img src="figs/girls_original.png"><figcaption>Original.</figcaption></figure>
        <figure><img src="figs/girls_blurred.png"><figcaption>Blurred.</figcaption></figure>
        <figure><img src="figs/girls_recovered.png"><figcaption>Unsharp “recovered”.</figcaption></figure>
      </div>
    
      <div class="hr"></div>
    </section>

    <!-- ===================== Part 2.2 ===================== -->
    <section id="part22">
      <h2>Part 2.2 — Hybrid Images</h2>
      <p>Low frequencies from one image + high frequencies from another.</p>

      <div class="grid">
        <figure><img src="mom.jpg"><figcaption>Mom (low frequencies).</figcaption></figure>
        <figure><img src="dad.jpg"><figcaption>Dad (high frequencies).</figcaption></figure>
        <figure><img src="figs/mom_dad_hybrid.png"><figcaption>Hybrid result.</figcaption></figure>
      </div>

      <div class="hr"></div>
    </section>

        <!-- ===================== Part 2.3 ===================== -->
        <section id="part23">
          <h2>Part 2.3 — Gaussian & Laplacian Stacks</h2>
          <p>
            I implemented functions <code>gaussian_stack(img, levels, sigma)</code> and 
            <code>laplacian_stack(img, levels, sigma)</code>. Gaussian stacks are created by repeatedly 
            blurring the image, while Laplacian stacks are formed by subtracting consecutive Gaussian levels. 
            The last Laplacian level holds the most blurred residual. 
          </p>
    
          <pre><code>def gaussian_stack(img, levels, sigma):
        stack = [img.copy()]
        current = img.copy()
        for i in range(levels - 1):
            current = gaussian_blur(current, sigma)
            stack.append(current.copy())
        return stack
    
    def laplacian_stack(img, levels, sigma):
        gauss_stack = gaussian_stack(img, levels, sigma)
        lap_stack = []
        for i in range(levels - 1):
            lap_stack.append(gauss_stack[i] - gauss_stack[i+1])
        lap_stack.append(gauss_stack[-1])
        return lap_stack</code></pre>
    
          <div class="grid">
            <figure><img src="figs/laplacian_stacks_oraple.png"><figcaption>
              Visualization of Gaussian & Laplacian stacks for Apple/Orange, matching Figure 3.42. 
              Shows high, medium, low frequency bands and reconstruction.
            </figcaption></figure>
          </div>
    
          <p class="muted">
            Each Gaussian level is smoother than the last, while each Laplacian level isolates 
            different frequency bands. Summing across all Laplacian levels reconstructs the 
            original image. These stacks are the foundation for the seamless blending in Part 2.4.
          </p>
    
          <div class="hr"></div>
        </section>


    <!-- ===================== Part 2.4 ===================== -->
    <section id="part24">
      <h2>Part 2.4 — Multiresolution Blending</h2>
    
      <h3>Oraple (Vertical Seam)</h3>
      <div class="grid">
        <figure><img src="spline/apple.jpeg"><figcaption>Apple input.</figcaption></figure>
        <figure><img src="spline/orange.jpeg"><figcaption>Orange input.</figcaption></figure>
        <figure><img src="figs/oraple_vertical.png"><figcaption>Oraple blended result.</figcaption></figure>
      </div>
    
      <h3>Messi Hair + Pope Face (Irregular Mask)</h3>
      <div class="grid">
        <figure><img src="spline/messi.jpg"><figcaption>Messi input.</figcaption></figure>
        <figure><img src="spline/pope.jpg"><figcaption>Pope input.</figcaption></figure>
        <figure><img src="spline/mask.png"><figcaption>Mask input.</figcaption></figure>
        <figure><img src="figs/messi_pope_blend.png"><figcaption>Messi Hair + Pope Face blend.</figcaption></figure>
      </div>
    
      <h3>Cotton Candy Dress (Irregular Mask)</h3>
      <div class="grid">
        <figure><img src="spline/cotton_candy.png"><figcaption>Cotton Candy texture.</figcaption></figure>
        <figure><img src="spline/dress.png"><figcaption>Dress base image.</figcaption></figure>
        <figure><img src="spline/dress_mask.png"><figcaption> Dress mask input.</figcaption></figure>
        <figure><img src="figs/cotton_candy_dress.png"><figcaption>Cotton Candy Dress blended result.</figcaption></figure>
      </div>
    
      <p class="muted">
        I use <strong>Laplacian stacks</strong> for each input and a <strong>Gaussian stack</strong> of the mask.
        At every level \(i\): <code>blend_i = mask_i · LapA_i + (1 - mask_i) · LapB_i</code>.
        The final result is the sum over all blended levels. This follows Burt &amp; Adelson’s multiresolution blending and
        smooths the transition across frequencies (fine detail to coarse structure).
      </p>
    
      <div class="hr"></div>
    </section>

  </main>
</body>
</html>
