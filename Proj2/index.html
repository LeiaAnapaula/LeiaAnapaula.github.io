<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>CS180 – Project 2 (Leia)</title>
  <style>
    :root { --w: 320px; --gap: 18px; }
    body { font-family: system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, sans-serif; line-height: 1.45; margin: 0; color: #222; }
    header { padding: 28px 16px; border-bottom: 1px solid #eee; }
    main { max-width: 960px; margin: 0 auto; padding: 24px 16px 64px; }
    h1, h2, h3 { margin: 0.4em 0 0.35em; }
    section { margin: 36px 0 44px; }
    .grid { display: grid; grid-template-columns: repeat(auto-fill, minmax(var(--w), 1fr)); gap: var(--gap); align-items: start; }
    figure { margin: 0; }
    figure img { width: 100%; height: auto; display: block; border-radius: 8px; }
    figure figcaption { font-size: 0.95rem; color: #555; margin-top: 6px; }
    pre { background: #0f172a; color: #e2e8f0; padding: 12px 14px; border-radius: 8px; overflow-x: auto; font-size: 0.9rem; }
    code { font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace; }
    .muted { color: #666; font-size: 0.95rem; }
    .pill { display: inline-block; background: #eef2ff; color: #3730a3; padding: 2px 8px; border-radius: 999px; font-size: 0.8rem; margin-left: 6px; }
    .hr { height: 1px; background: #eee; margin: 28px 0; }
  </style>
</head>
<body>
  <header>
    <h1>Project 2 – Fun with Filters and Frequencies <span class="pill">Leia</span></h1>
    <p class="muted">CS180/280A • Fall 2025</p>
  </header>

  <main>
    <!-- ===================== Part 1.1 ===================== -->
    <section id="part11">
      <h2>Part 1.1 — Convolutions from Scratch</h2>
      <p>
        I implemented 2D convolution two ways: a <em>4-loop</em> version (over <code>H, W, kh, kw</code>) and a
        faster <em>2-loop</em> version (loop over pixels and take an elementwise dot with the local patch).
        As a sanity check, I compared to <code>scipy.signal.convolve2d</code> (MAE ≈ 1e-7).
      </p>

      <div class="grid">
        <figure>
          <img src="figs/blur9_selfie.png" alt="Selfie blurred with a 9x9 box filter">
          <figcaption>Selfie with 9×9 box filter (averaging). Smooths fine detail by design.</figcaption>
        </figure>
      </div>

      <h3>Key snippets</h3>
      <p class="muted">4-loop convolution (kernel is flipped internally for true convolution):</p>
      <pre><code># 4-loop conv (numpy only)
def conv2d_naive_4loops(img, kernel):
    img = img.astype(np.float32)
    k = np.array(kernel, np.float32)[::-1, ::-1]          # flip
    H, W = img.shape; kh, kw = k.shape
    ph, pw = kh//2, kw//2
    p = np.pad(img, ((ph, ph), (pw, pw)), mode='constant')
    out = np.zeros((H, W), np.float32)
    for i in range(H):
        for j in range(W):
            acc = 0.0
            for u in range(kh):
                for v in range(kw):
                    acc += p[i+u, j+v] * k[u, v]
            out[i, j] = acc
    return out</code></pre>

      <p class="muted">2-loop version (vectorized inner patch):</p>
      <pre><code>def conv2d_naive_2loops(img, kernel):
    img = img.astype(np.float32)
    k = np.array(kernel, np.float32)[::-1, ::-1]
    H, W = img.shape; kh, kw = k.shape
    ph, pw = kh//2, kw//2
    p = np.pad(img, ((ph, ph), (pw, pw)), mode='constant')
    out = np.zeros((H, W), np.float32)
    for i in range(H):
        for j in range(W):
            region = p[i:i+kh, j:j+kw]
            out[i, j] = np.sum(region * k)
    return out</code></pre>

      <div class="hr"></div>
    </section>

    <!-- ===================== Part 1.2 ===================== -->
    <section id="part12">
      <h2>Part 1.2 — Finite Difference Operator</h2>
      <p>
        I convolved <em>cameraman</em> with finite-difference filters
        \(D_x=[1,-1]\) and \(D_y=\begin{bmatrix}1\\-1\end{bmatrix}\) using <code>scipy.signal.convolve2d</code>,
        then formed the gradient magnitude \( \sqrt{G_x^2 + G_y^2} \) and thresholded it to make a binary edge map.
      </p>

      <div class="grid">
        <figure>
          <img src="figs/cameraman_gx.png" alt="Gx">
          <figcaption>Horizontal derivative \(G_x\) (sensitive to vertical edges).</figcaption>
        </figure>
        <figure>
          <img src="figs/cameraman_gy.png" alt="Gy">
          <figcaption>Vertical derivative \(G_y\) (sensitive to horizontal edges).</figcaption>
        </figure>
        <figure>
          <img src="figs/cameraman_mag.png" alt="Gradient magnitude">
          <figcaption>Gradient magnitude \(|\nabla f|\).</figcaption>
        </figure>
        <figure>
          <img src="figs/cameraman_edges.png" alt="Binarized edges">
          <figcaption>Edges after thresholding (picked qualitatively to keep real edges, suppress noise).</figcaption>
        </figure>
      </div>

      <div class="hr"></div>
    </section>

    <!-- ===================== Part 1.3 ===================== -->
    <section id="part13">
      <h2>Part 1.3 — Derivative of Gaussian (DoG) Filter</h2>
      <p>
        To reduce noise, I first smooth with a Gaussian and then differentiate (equivalently,
        convolve with DoG kernels). The DoG edges look much cleaner than raw finite-difference edges
        because the Gaussian pre-smoothing suppresses high-frequency noise before edge detection.
      </p>

      <div class="grid">
        <figure>
          <img src="figs/cameraman_dogx.png" alt="DoG Gx">
          <figcaption>DoG \(G_x\) after Gaussian smoothing.</figcaption>
        </figure>
        <figure>
          <img src="figs/cameraman_dogy.png" alt="DoG Gy">
          <figcaption>DoG \(G_y\) after Gaussian smoothing.</figcaption>
        </figure>
        <figure>
          <img src="figs/cameraman_dogmag.png" alt="DoG magnitude">
          <figcaption>DoG gradient magnitude \(|\nabla f|\).</figcaption>
        </figure>
        <figure>
          <img src="figs/cameraman_dogf.png" alt="DoG edges">
          <figcaption>DoG edges - cleaner and more contiguous than Part 1.2.</figcaption>
        </figure>
      </div>

      <p class="muted">
        Reflection: Compared to Part 1.2, the DoG approach reduces noise and produces more stable, 
        contiguous edges. The Gaussian smoothing with appropriate σ suppresses texture while preserving 
        important structural edges.
      </p>
      
      <div class="hr"></div>
    </section>

    <!-- ===================== Part 2.1 ===================== -->
    <section id="part21">
      <h2>Part 2.1 — Image "Sharpening"</h2>
      <p>
        I implemented the unsharp masking technique: extract high frequencies by subtracting a 
        Gaussian-blurred version from the original, then add them back with amplification. 
        The formula is: <code>sharpened = image + α × (image - blur)</code>, where α controls 
        sharpening intensity.
      </p>

      <h3>Taj Mahal</h3>
      <div class="grid">
        <figure>
          <img src="taj.jpg" alt="Taj original">
          <figcaption>Taj Mahal (original).</figcaption>
        </figure>
        <figure>
          <img src="figs/taj_unsharp.png" alt="Taj sharpened">
          <figcaption>Taj Mahal sharpened (α=1.2, σ=1.2).</figcaption>
        </figure>
      </div>

      <h3>Siblings</h3>
      <div class="grid">
        <figure>
          <img src="siblings.jpg" alt="Siblings original">
          <figcaption>Siblings (original).</figcaption>
        </figure>
        <figure>
          <img src="figs/siblings_unsharp.png" alt="Siblings sharpened">
          <figcaption>Siblings sharpened (α=1.5, σ=1.0).</figcaption>
        </figure>
      </div>

      <h3>Evaluation: Blur then Sharpen</h3>
      <p>
        To test the limits of unsharp masking, I took a sharp image, artificially blurred it, 
        then attempted to recover sharpness. Results show that while unsharp masking can enhance 
        perceived sharpness, it cannot fully recover lost detail—information destroyed by blurring 
        is gone forever.
      </p>

      <div class="grid">
        <figure>
          <img src="figs/girls_original.png" alt="Original sharp">
          <figcaption>Original sharp image.</figcaption>
        </figure>
        <figure>
          <img src="figs/girls_blurred.png" alt="Blurred">
          <figcaption>Artificially blurred (σ=2.0).</figcaption>
        </figure>
        <figure>
          <img src="figs/girls_recovered.png" alt="Recovered">
          <figcaption>After unsharp masking—sharper but not identical to original.</figcaption>
        </figure>
      </div>

      <div class="hr"></div>
    </section>

    <!-- ===================== Part 2.2 ===================== -->
    <section id="part22">
      <h2>Part 2.2 — Hybrid Images</h2>
      <p>
        Following Oliva, Torralba, and Schyns (SIGGRAPH 2006), I created hybrid images by combining 
        low frequencies from one image with high frequencies from another. When viewed up close, 
        the high-frequency image dominates; from far away, only the low-frequency image is visible.
      </p>

      <h3>Mom + Dad Hybrid</h3>
      <div class="grid">
        <figure>
          <img src="mom.jpg" alt="Mom">
          <figcaption>Mom (will be low frequencies—visible close).</figcaption>
        </figure>
        <figure>
          <img src="dad.jpg" alt="Dad">
          <figcaption>Dad (will be high frequencies—visible far).</figcaption>
        </figure>
        <figure>
          <img src="figs/mom_dad_hybrid.png" alt="Mom+Dad hybrid">
          <figcaption>Hybrid result. Step back to see Dad!</figcaption>
        </figure>
      </div>

      <h3>Frequency Analysis</h3>
      <p class="muted">
        FFT magnitude visualizations show how the hybrid combines low frequencies (concentrated 
        in the center) from one image with high frequencies (spread throughout) from another.
        The log magnitude plots demonstrate the frequency separation clearly.
      </p>

      <div class="hr"></div>
    </section>

    <section>
      <p class="muted"><a href="../index.html">← Back to Portfolio</a></p>
    </section>
  </main>
</body>
</html>
