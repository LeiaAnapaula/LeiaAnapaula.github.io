<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>CS180 – Project 4 by Leia</title>
  <style>
    :root { --w: 320px; --gap: 18px; }
    body { font-family: system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, sans-serif; line-height: 1.45; margin: 0; color: #222; }
    header { padding: 28px 16px; border-bottom: 1px solid #eee; }
    main { max-width: 1200px; margin: 0 auto; padding: 24px 16px 64px; }
    h1, h2, h3 { margin: 0.4em 0 0.35em; }
    section { margin: 36px 0 44px; }
    .grid { display: grid; grid-template-columns: repeat(auto-fill, minmax(var(--w), 1fr)); gap: var(--gap); align-items: start; }
    .grid-2 { grid-template-columns: repeat(2, 1fr); }
    .grid-3 { grid-template-columns: repeat(3, 1fr); }
    figure { margin: 0; }
    figure img { width: 100%; height: auto; display: block; border-radius: 8px; }
    figure figcaption { font-size: 0.95rem; color: #555; margin-top: 6px; }
    pre { background: #0f172a; color: #e2e8f0; padding: 12px 14px; border-radius: 8px; overflow-x: auto; font-size: 0.9rem; }
    code { font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace; }
    .muted { color: #666; font-size: 0.95rem; }
    .pill { display: inline-block; background: #eef2ff; color: #3730a3; padding: 2px 8px; border-radius: 999px; font-size: 0.8rem; margin-left: 6px; }
    .hr { height: 1px; background: #eee; margin: 28px 0; }
    table { width: 100%; border-collapse: collapse; margin: 16px 0; }
    th, td { padding: 8px 12px; text-align: left; border-bottom: 1px solid #eee; }
    th { background: #f8fafc; font-weight: 600; }
  </style>
</head>
<body>
  <header>
    <h1>Project 4 - Neural Radiance Fields <span class="pill">Leia</span></h1>
    <p class="muted">CS180 • Fall 2025</p>
  </header>

  <main>
    <!-- ===================== Part 0 ===================== -->
    <section id="a1">
      <h2>Part 0.1: Calibrating Your Camera</h2>
      <p>
        I captured two sets of image pairs with projective transformations by fixing the center 
        of projection and rotating the camera. Each pair has 40-70% overlap for robust registration.
      </p>

      <h3>Part 0.2: Capturing a 3D Object Scan (Lafufu)</h3>
      <div class="grid grid-2">
        <figure>
          <img src="Part 0.2 - Lafufu Calibration.png" alt="Lafufu checkerboard calibration">
          <figcaption>Lafufu – checkerboard calibration view.</figcaption>
        </figure>
        <figure>
          <img src="Part 0.2 - Lafufu Poses.png" alt="Lafufu camera poses visualization">
          <figcaption>Lafufu – recovered camera poses for the calibration sequence.</figcaption>
        </figure>
      </div>

      <h3>Part 0.3: Estimating Camera Pose</h3>
      <div class="grid grid-2">
        <figure><img src="assets/imgs/hearst_left.png"><figcaption>Hearst Left</figcaption></figure>
        <figure><img src="assets/imgs/hearst_right.png"><figcaption>Hearst Right</figcaption></figure>
      </div>
      <p class="muted">
        Fixed COP, handheld rotation (~60% overlap). Hearst Mining Building facade.
      </p>

      <h3>Part 0.4: Undistorting images and creating a dataset</h3>
      <div class="grid grid-2">
        <figure><img src="assets/imgs/hearst_left.png"><figcaption>Hearst Left</figcaption></figure>
        <figure><img src="assets/imgs/hearst_right.png"><figcaption>Hearst Right</figcaption></figure>
      </div>
      <p class="muted">
        Fixed COP, handheld rotation (~60% overlap). Hearst Mining Building facade.
      </p>

      <div class="hr"></div>
    </section>

    <!-- ===================== Part 1 ===================== -->
    <h2>Part 1: Fitting a Neural Field to a 2D Image</h2>
    <p>
      I fit an MLP to a single 2D image of a fox by treating each pixel coordinate as input
      and predicting its RGB value.
    </p>
    
    <div class="grid grid-1">
      <figure>
        <img src="Part 1 fox pixels.png" alt="Fox image reconstructed from neural field">
        <figcaption>
          Neural field fit to the fox image. The network learns a continuous mapping from
          (x, y) pixel coordinates to RGB colors.
        </figcaption>
      </figure>
    </div>

        <!-- ===================== Part 2 ===================== -->
    <section id="a3">
      <h2>Part 2.1: Creating Rays from Cameras</h2>
      <p>
        For each training image I convert camera intrinsics and extrinsics into a dense grid of
        <em>rays</em>. A ray is defined by an origin <code>o</code> (camera center in world space) and
        a direction <code>d</code> (unprojected pixel through the camera). This lets the network operate
        directly in 3D, independent of image resolution.
      </p>

      <pre><code># get_rays: K is intrinsics, c2w is camera-to-world transform
def get_rays(H, W, K, c2w):
    i, j = torch.meshgrid(
        torch.arange(W, device=K.device),
        torch.arange(H, device=K.device),
        indexing="xy",
    )
    pixels = torch.stack([(i - K[0, 2]) / K[0, 0],
                          (j - K[1, 2]) / K[1, 1],
                          torch.ones_like(i)], dim=-1)  # (H, W, 3)

    # Rotate into world space and normalize
    dirs = (pixels[..., None, :] @ c2w[:3, :3].T)[..., 0]
    dirs = dirs / torch.norm(dirs, dim=-1, keepdim=True)

    # Same origin for all pixels: camera center in world coords
    origins = c2w[:3, 3].expand_as(dirs)
    return origins, dirs</code></pre>

      <div class="grid grid-2">
        <figure>
          <img src="assets/imgs/lego_train_sample.png" alt="Lego training image">
          <figcaption>Example training image from the Lego multi-view dataset.</figcaption>
        </figure>
        <figure>
          <img src="assets/imgs/rays_3d_vis.png" alt="3D rays visualization">
          <figcaption>3D visualization of rays emitted from one camera into the scene.</figcaption>
        </figure>
      </div>

      <p class="muted">
        Each pixel now corresponds to a ray in world space. These rays drive all later steps:
        sampling points along them, querying the NeRF network, and volume rendering back into
        pixel colors.
      </p>

      <div class="hr"></div>

      <h2>Part 2.2: Sampling Points Along Rays</h2>
      <p>
        Along each ray I sample <code>N=64</code> points between near and far bounds
        (<code>t ∈ [2.0, 6.0]</code>). The sampling is stratified: I divide the interval into equal
        bins and jitter a single sample inside each bin. This reduces aliasing and gives smoother
        reconstructions.
      </p>

      <pre><code>def sample_points(rays_o, rays_d, N_samples, near=2.0, far=6.0):
    R = rays_o.shape[0]
    t_vals = torch.linspace(near, far, N_samples, device=rays_o.device)  # (N,)
    # Stratified jitter within each bin
    mids = 0.5 * (t_vals[:-1] + t_vals[1:])
    upper = torch.cat([mids, t_vals[-1:]], dim=0)
    lower = torch.cat([t_vals[:1], mids], dim=0)
    t_rand = torch.rand((R, N_samples, 1), device=rays_o.device)
    t = (lower[None, :, None] + (upper - lower)[None, :, None] * t_rand)
    # 3D positions along the ray
    pts = rays_o[:, None, :] + rays_d[:, None, :] * t
    step_size = (far - near) / N_samples
    return pts, t, step_size</code></pre>

      <div class="grid grid-2">
        <figure>
          <img src="assets/imgs/rays_samples_vis.png" alt="Rays and samples visualization">
          <figcaption>Visualization of up to 100 rays and their sampled 3D points.</figcaption>
        </figure>
        <figure>
          <img src="assets/imgs/depth_debug_vis.png" alt="Depth along rays">
          <figcaption>Color-coded depth values for sampled points along each ray.</figcaption>
        </figure>
      </div>

      <p class="muted">
        Sampling points turns each ray into a small 1D volume. The NeRF network predicts color and
        density at these points, which we then integrate with volume rendering.
      </p>

      <div class="hr"></div>

      <h2>Part 2.3: Visualizing Cameras, Rays, and Samples</h2>
      <p>
        Here I visualize the camera frustums, a subset of rays, and the sampled points used
        to train the NeRF on the Lego scene.
      </p>
      
      <div class="grid grid-3">
        <figure>
          <img src="Part 2.3 deliverable.png" alt="Camera frustums and rays visualization">
          <figcaption>Camera frustums and a subset of rays in the Lego scene.</figcaption>
        </figure>
      
        <figure>
          <img src="part 2.3 deliverable 2.png" alt="Ray samples close-up view">
          <figcaption>Close-up of sampled points along rays.</figcaption>
        </figure>
      
        <figure>
          <img src="part 2.3 deliverable 3.png" alt="Alternative view of rays and samples">
          <figcaption>Alternative viewpoint of cameras and ray geometry.</figcaption>
        </figure>
      </div>

      <h2>Part 2.5: Volume Rendering</h2>
      <p>
        Given per-point densities <code>σᵢ</code> and colors <code>cᵢ</code> along each ray, I implement
        the discrete volume rendering equation in PyTorch. The key idea is to treat the ray as a
        semi-transparent volume and compute how much light is absorbed and emitted at each step.
      </p>

      <pre><code>def volrend(sigmas, rgbs, step_size):
    """
    sigmas: (B, N, 1) densities along each ray
    rgbs:   (B, N, 3) colors at those samples
    step_size: scalar distance between samples
    returns: (B, 3) rendered colors
    """
    sigma_delta = sigmas * step_size                 # (B, N, 1)
    alphas = 1.0 - torch.exp(-sigma_delta)          # αᵢ = 1 - exp(-σᵢ δ)

    cumsum_sigma_delta = torch.cumsum(sigma_delta, dim=1)
    accum_before = cumsum_sigma_delta - sigma_delta # ∑_{j&lt;i} σⱼ δ

    T = torch.exp(-accum_before)                    # Tᵢ = exp(-∑_{j&lt;i} σⱼ δ)
    weights = T * alphas                            # wᵢ = Tᵢ αᵢ

    return torch.sum(weights * rgbs, dim=1)         # ∑ wᵢ cᵢ</code></pre>

      <p class="muted">
        Intuitively, <code>Tᵢ</code> is the probability the ray has not terminated before sample i,
        and <code>αᵢ</code> is the probability it terminates <em>at</em> i. Their product gives a
        weight for each sample, and summing the weighted colors yields the final pixel color. This
        function is fully differentiable and passes the provided assertion test.
      </p>

      <div class="grid grid-3">
        <figure>
          <img src="assets/imgs/volume_diagram.png" alt="Volume rendering diagram">
          <figcaption>Diagram of transmittance and alpha along a single ray.</figcaption>
        </figure>
        <figure>
          <img src="assets/imgs/volrend_unit_test.png" alt="Unit test for volrend">
          <figcaption>Unit test comparison: my implementation matches the reference values.</figcaption>
        </figure>
        <figure>
          <img src="assets/imgs/volrend_debug_render.png" alt="Debug render">
          <figcaption>Debug render of randomly initialized network using the volume renderer.</figcaption>
        </figure>
      </div>

      <div class="hr"></div>
    </section>

    <!-- ===================== Part 2.6 ===================== -->
<section id="a4">
  <h2>Part 2.6: Training NeRF on My Own Captured Object</h2>
  <p>
    Instead of the provided Lego dataset, I captured my own multi-view scene using a drink cup placed
    on a sheet of white paper. I moved around the object in a circular path and recorded images with
    noticeable parallax. These images became the training set for my NeRF.
  </p>

  <div class="grid grid-2">
    <figure>
      <img src="assets/imgs/drink_raw_01.jpg">
      <figcaption>Example raw training image.</figcaption>
    </figure>
    <figure>
      <img src="assets/imgs/drink_raw_02.jpg">
      <figcaption>Another view from the capture sequence.</figcaption>
    </figure>
  </div>

  <p class="muted">
    I estimated camera intrinsics/extrinsics using COLMAP, converted intrinsics to the NeRF
    coordinate system, and generated rays for each pixel exactly as in Part 2.1.
    This produced a complete multi-view dataset of my drink for NeRF training.
  </p>

  <h3>Training Setup</h3>
  <p>
    I train using batches of <strong>10,000 random rays</strong> per step, each with
    <strong>64 stratified samples</strong> along the ray. For each batch:
  </p>

  <ol>
    <li>Randomly sample rays across all cameras.</li>
    <li>Sample positions <code>t</code> along each ray.</li>
    <li>Query the NeRF MLP to get <code>(σᵢ, cᵢ)</code>.</li>
    <li>Apply my volume renderer (Part 2.5) to integrate along the ray.</li>
    <li>Compare with ground-truth pixel colors using MSE loss.</li>
  </ol>

  <pre><code>loss = F.mse_loss(pred_rgb, target_rgb)
optimizer.zero_grad()
loss.backward()
optimizer.step()</code></pre>

  <div class="grid grid-3">
    <figure>
      <img src="assets/imgs/drink_iter_000.png">
      <figcaption>Iteration 0 — network untrained.</figcaption>
    </figure>
    <figure>
      <img src="assets/imgs/drink_iter_200.png">
      <figcaption>Iteration 200 — basic silhouette forms.</figcaption>
    </figure>
    <figure>
      <img src="assets/imgs/drink_iter_1000.png">
      <figcaption>Iteration 1000 — cup geometry and texture appear.</figcaption>
    </figure>
  </div>

  <h3>PSNR on My Custom Validation Set</h3>
  <figure>
    <img src="assets/imgs/drink_psnr_curve.png">
    <figcaption>
      PSNR curve computed on six held-out camera poses from my drink dataset.
      The model steadily improves and reaches &gt; 23 dB.
    </figcaption>
  </figure>

  <h3>Novel Views + Spherical Rendering</h3>
  <p>
    After training, I rendered novel viewpoints of the cup by placing virtual cameras in a circular
    orbit around the object. This demonstrates that the NeRF learned a coherent 3D representation.
  </p>

  <div class="grid grid-3">
    <figure>
      <img src="assets/imgs/drink_novel_01.png">
      <figcaption>Novel view 1.</figcaption>
    </figure>
    <figure>
      <img src="assets/imgs/drink_novel_02.png">
      <figcaption>Novel view 2.</figcaption>
    </figure>
    <figure>
      <img src="assets/imgs/drink_spherical_frame.png">
      <figcaption>Frame from spherical rendering.</figcaption>
    </figure>
  </div>

  <p class="muted">
    The final video (drink_spherical.mp4) shows a smooth orbit around the cup, including consistent
    occlusion, shadows, and geometry. The network successfully reconstructs my custom scene.
  </p>

  <div class="hr"></div>
</section>

    
  </main>
</body>
</html>
