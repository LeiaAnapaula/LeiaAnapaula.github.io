<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>CS180 – Project 4 by Leia</title>
  <style>
    :root { --w: 320px; --gap: 18px; }
    body { font-family: system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, sans-serif; line-height: 1.45; margin: 0; color: #222; }
    header { padding: 28px 16px; border-bottom: 1px solid #eee; }
    main { max-width: 1200px; margin: 0 auto; padding: 24px 16px 64px; }
    h1, h2, h3 { margin: 0.4em 0 0.35em; }
    section { margin: 36px 0 44px; }
    .grid { display: grid; grid-template-columns: repeat(auto-fill, minmax(var(--w), 1fr)); gap: var(--gap); align-items: start; }
    .grid-2 { grid-template-columns: repeat(2, 1fr); }
    .grid-3 { grid-template-columns: repeat(3, 1fr); }
    figure { margin: 0; }
    figure img { width: 100%; height: auto; display: block; border-radius: 8px; }
    figure figcaption { font-size: 0.95rem; color: #555; margin-top: 6px; }
    pre { background: #0f172a; color: #e2e8f0; padding: 12px 14px; border-radius: 8px; overflow-x: auto; font-size: 0.9rem; }
    code { font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace; }
    .muted { color: #666; font-size: 0.95rem; }
    .pill { display: inline-block; background: #eef2ff; color: #3730a3; padding: 2px 8px; border-radius: 999px; font-size: 0.8rem; margin-left: 6px; }
    .hr { height: 1px; background: #eee; margin: 28px 0; }
    table { width: 100%; border-collapse: collapse; margin: 16px 0; }
    th, td { padding: 8px 12px; text-align: left; border-bottom: 1px solid #eee; }
    th { background: #f8fafc; font-weight: 600; }
  </style>
</head>
<body>
  <header>
    <h1>Project 4 - Neural Radiance Fields <span class="pill">Leia</span></h1>
    <p class="muted">CS180 • Fall 2025</p>
  </header>

  <main>
    <!-- ===================== Part 0 ===================== -->
    <section id="a1">
      <h2>Part 0.1: Calibrating Your Camera</h2>
      <p>
        I captured two sets of image pairs with projective transformations by fixing the center 
        of projection and rotating the camera. Each pair has 40-70% overlap for robust registration.
      </p>

      <h3>Part 0.2: Capturing a 3D Object Scan</h3>
      <div class="grid grid-2">
        <figure><img src="assets/imgs/left_a1.png"><figcaption>Wheeler Left</figcaption></figure>
        <figure><img src="assets/imgs/right_a1.png"><figcaption>Wheeler Right</figcaption></figure>
      </div>
      <p class="muted">
        Fixed COP, handheld rotation (~55% overlap). Wheeler Hall exterior with architectural details.
      </p>

      <h3>Part 0.3: Estimating Camera Pose</h3>
      <div class="grid grid-2">
        <figure><img src="assets/imgs/hearst_left.png"><figcaption>Hearst Left</figcaption></figure>
        <figure><img src="assets/imgs/hearst_right.png"><figcaption>Hearst Right</figcaption></figure>
      </div>
      <p class="muted">
        Fixed COP, handheld rotation (~60% overlap). Hearst Mining Building facade.
      </p>

      <h3>Part 0.4: Undistorting images and creating a dataset</h3>
      <div class="grid grid-2">
        <figure><img src="assets/imgs/hearst_left.png"><figcaption>Hearst Left</figcaption></figure>
        <figure><img src="assets/imgs/hearst_right.png"><figcaption>Hearst Right</figcaption></figure>
      </div>
      <p class="muted">
        Fixed COP, handheld rotation (~60% overlap). Hearst Mining Building facade.
      </p>

      <div class="hr"></div>
    </section>

    <!-- ===================== Part 1 ===================== -->
    <section id="a2">
      <h2>Fit a Neural Field to a 2D Image</h2>
      <p>
        Implemented <code>computeH(im1_pts, im2_pts)</code> using the <strong>Ah=b formulation</strong> 
        with 8 degrees of freedom (H[2,2]=1 fixed). The function sets up a linear system from point 
        correspondences and solves using least-squares.
      </p>

      <pre><code>def computeH(im1_points, im2_points):
    """
    Compute homography H: im2_points = H @ im1_points
    Uses Ah=b formulation with least-squares.
    """
    n = im1_points.shape[0]
    A = np.zeros((2*n, 8))
    b = np.zeros((2*n, 1))
    
    for i in range(n):
        x, y = im1_points[i]
        xp, yp = im2_points[i]
        
        # Row for x' equation
        A[2*i] = [x, y, 1, 0, 0, 0, -x*xp, -y*xp]
        b[2*i] = xp
        
        # Row for y' equation  
        A[2*i+1] = [0, 0, 0, x, y, 1, -x*yp, -y*yp]
        b[2*i+1] = yp
    
    # Solve least squares
    h, _, _, _ = np.linalg.lstsq(A, b, rcond=None)
    H = np.vstack([h, [[1]]]).reshape(3, 3)
    return H</code></pre>

      <h3>Wheeler Correspondences</h3>
      <figure><img src="assets/imgs/A2_wheeler_correspondences.png"><figcaption>
        8-point correspondences visualized. Color-coded lines connect matching features between left and right views.
      </figcaption></figure>

      <table>
        <thead>
          <tr><th colspan="3">Recovered Homography Matrix (RIGHT → LEFT)</th></tr>
        </thead>
        <tbody>
          <tr><td>0.7234</td><td>-0.0891</td><td>1156.32</td></tr>
          <tr><td>0.1023</td><td>0.8945</td><td>-234.67</td></tr>
          <tr><td>0.0001</td><td>-0.0000</td><td>1.0000</td></tr>
        </tbody>
      </table>
      <p class="muted"><strong>Reprojection Error:</strong> RMSE = 2.34 pixels, Median = 1.89 pixels</p>

      <h3>Hearst Correspondences</h3>
      <figure><img src="assets/imgs/A2_hearst_correspondences.png"><figcaption>
        8 point correspondences for Hearst dataset. Numbered markers show matching features.
      </figcaption></figure>

      <table>
        <thead>
          <tr><th colspan="3">Recovered Homography Matrix (RIGHT → LEFT)</th></tr>
        </thead>
        <tbody>
          <tr><td>0.8912</td><td>0.0456</td><td>789.45</td></tr>
          <tr><td>-0.0234</td><td>0.9234</td><td>-123.89</td></tr>
          <tr><td>0.0000</td><td>0.0000</td><td>1.0000</td></tr>
        </tbody>
      </table>
      <p class="muted"><strong>Reprojection Error:</strong> RMSE = 3.12 pixels, Median = 2.67 pixels</p>

      <div class="hr"></div>
    </section>

        <!-- ===================== Part 2 ===================== -->
    <section id="a3">
      <h2>Part 2.1: Creating Rays from Cameras</h2>
      <p>
        For each training image I convert camera intrinsics and extrinsics into a dense grid of
        <em>rays</em>. A ray is defined by an origin <code>o</code> (camera center in world space) and
        a direction <code>d</code> (unprojected pixel through the camera). This lets the network operate
        directly in 3D, independent of image resolution.
      </p>

      <pre><code># get_rays: K is intrinsics, c2w is camera-to-world transform
def get_rays(H, W, K, c2w):
    i, j = torch.meshgrid(
        torch.arange(W, device=K.device),
        torch.arange(H, device=K.device),
        indexing="xy",
    )
    pixels = torch.stack([(i - K[0, 2]) / K[0, 0],
                          (j - K[1, 2]) / K[1, 1],
                          torch.ones_like(i)], dim=-1)  # (H, W, 3)

    # Rotate into world space and normalize
    dirs = (pixels[..., None, :] @ c2w[:3, :3].T)[..., 0]
    dirs = dirs / torch.norm(dirs, dim=-1, keepdim=True)

    # Same origin for all pixels: camera center in world coords
    origins = c2w[:3, 3].expand_as(dirs)
    return origins, dirs</code></pre>

      <div class="grid grid-2">
        <figure>
          <img src="assets/imgs/lego_train_sample.png" alt="Lego training image">
          <figcaption>Example training image from the Lego multi-view dataset.</figcaption>
        </figure>
        <figure>
          <img src="assets/imgs/rays_3d_vis.png" alt="3D rays visualization">
          <figcaption>3D visualization of rays emitted from one camera into the scene.</figcaption>
        </figure>
      </div>

      <p class="muted">
        Each pixel now corresponds to a ray in world space. These rays drive all later steps:
        sampling points along them, querying the NeRF network, and volume rendering back into
        pixel colors.
      </p>

      <div class="hr"></div>

      <h2>Part 2.2: Sampling Points Along Rays</h2>
      <p>
        Along each ray I sample <code>N=64</code> points between near and far bounds
        (<code>t ∈ [2.0, 6.0]</code>). The sampling is stratified: I divide the interval into equal
        bins and jitter a single sample inside each bin. This reduces aliasing and gives smoother
        reconstructions.
      </p>

      <pre><code>def sample_points(rays_o, rays_d, N_samples, near=2.0, far=6.0):
    R = rays_o.shape[0]
    t_vals = torch.linspace(near, far, N_samples, device=rays_o.device)  # (N,)
    # Stratified jitter within each bin
    mids = 0.5 * (t_vals[:-1] + t_vals[1:])
    upper = torch.cat([mids, t_vals[-1:]], dim=0)
    lower = torch.cat([t_vals[:1], mids], dim=0)
    t_rand = torch.rand((R, N_samples, 1), device=rays_o.device)
    t = (lower[None, :, None] + (upper - lower)[None, :, None] * t_rand)
    # 3D positions along the ray
    pts = rays_o[:, None, :] + rays_d[:, None, :] * t
    step_size = (far - near) / N_samples
    return pts, t, step_size</code></pre>

      <div class="grid grid-2">
        <figure>
          <img src="assets/imgs/rays_samples_vis.png" alt="Rays and samples visualization">
          <figcaption>Visualization of up to 100 rays and their sampled 3D points.</figcaption>
        </figure>
        <figure>
          <img src="assets/imgs/depth_debug_vis.png" alt="Depth along rays">
          <figcaption>Color-coded depth values for sampled points along each ray.</figcaption>
        </figure>
      </div>

      <p class="muted">
        Sampling points turns each ray into a small 1D volume. The NeRF network predicts color and
        density at these points, which we then integrate with volume rendering.
      </p>

      <div class="hr"></div>

      <h2>Part 2.3: Neural Radiance Field Network</h2>
      <p>
        I use a multi-layer perceptron that maps position (and viewing direction) to density and RGB.
        Inputs are first passed through a <strong>positional encoding</strong> to allow the MLP to
        represent high-frequency details.
      </p>

      <pre><code>class NeRF(nn.Module):
    def __init__(self, pos_dim=3, dir_dim=3, hidden_dim=256):
        super().__init__()
        in_pos = pos_dim * 2 * 10  # 10 frequency bands
        in_dir = dir_dim * 2 * 4   # 4 frequency bands

        self.mlp_pos = nn.Sequential(
            nn.Linear(in_pos, hidden_dim), nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),
        )
        self.sigma_head = nn.Linear(hidden_dim, 1)

        self.mlp_rgb = nn.Sequential(
            nn.Linear(hidden_dim + in_dir, hidden_dim), nn.ReLU(),
            nn.Linear(hidden_dim, 3), nn.Sigmoid()
        )

    def forward(self, pts_enc, dirs_enc):
        h = self.mlp_pos(pts_enc)
        sigma = F.relu(self.sigma_head(h))
        h_rgb = torch.cat([h, dirs_enc], dim=-1)
        rgb = self.mlp_rgb(h_rgb)
        return sigma, rgb</code></pre>

      <div class="grid grid-2">
        <figure>
          <img src="assets/imgs/network_diagram.png" alt="Network diagram">
          <figcaption>Diagram of the NeRF MLP: positional encoding → shared trunk → σ and RGB heads.</figcaption>
        </figure>
        <figure>
          <img src="assets/imgs/pe_vs_nope.png" alt="Effect of positional encoding">
          <figcaption>Without positional encoding the network blurs details; with it, high-frequency
          geometry and textures are preserved.</figcaption>
        </figure>
      </div>

      <div class="hr"></div>

      <h2>Part 2.5: Volume Rendering</h2>
      <p>
        Given per-point densities <code>σᵢ</code> and colors <code>cᵢ</code> along each ray, I implement
        the discrete volume rendering equation in PyTorch. The key idea is to treat the ray as a
        semi-transparent volume and compute how much light is absorbed and emitted at each step.
      </p>

      <pre><code>def volrend(sigmas, rgbs, step_size):
    """
    sigmas: (B, N, 1) densities along each ray
    rgbs:   (B, N, 3) colors at those samples
    step_size: scalar distance between samples
    returns: (B, 3) rendered colors
    """
    sigma_delta = sigmas * step_size                 # (B, N, 1)
    alphas = 1.0 - torch.exp(-sigma_delta)          # αᵢ = 1 - exp(-σᵢ δ)

    cumsum_sigma_delta = torch.cumsum(sigma_delta, dim=1)
    accum_before = cumsum_sigma_delta - sigma_delta # ∑_{j&lt;i} σⱼ δ

    T = torch.exp(-accum_before)                    # Tᵢ = exp(-∑_{j&lt;i} σⱼ δ)
    weights = T * alphas                            # wᵢ = Tᵢ αᵢ

    return torch.sum(weights * rgbs, dim=1)         # ∑ wᵢ cᵢ</code></pre>

      <p class="muted">
        Intuitively, <code>Tᵢ</code> is the probability the ray has not terminated before sample i,
        and <code>αᵢ</code> is the probability it terminates <em>at</em> i. Their product gives a
        weight for each sample, and summing the weighted colors yields the final pixel color. This
        function is fully differentiable and passes the provided assertion test.
      </p>

      <div class="grid grid-3">
        <figure>
          <img src="assets/imgs/volume_diagram.png" alt="Volume rendering diagram">
          <figcaption>Diagram of transmittance and alpha along a single ray.</figcaption>
        </figure>
        <figure>
          <img src="assets/imgs/volrend_unit_test.png" alt="Unit test for volrend">
          <figcaption>Unit test comparison: my implementation matches the reference values.</figcaption>
        </figure>
        <figure>
          <img src="assets/imgs/volrend_debug_render.png" alt="Debug render">
          <figcaption>Debug render of randomly initialized network using the volume renderer.</figcaption>
        </figure>
      </div>

      <div class="hr"></div>
    </section>

    <!-- ===================== Part 2.6 ===================== -->
<section id="a4">
  <h2>Part 2.6: Training NeRF on My Own Captured Object</h2>
  <p>
    Instead of the provided Lego dataset, I captured my own multi-view scene using a drink cup placed
    on a sheet of white paper. I moved around the object in a circular path and recorded images with
    noticeable parallax. These images became the training set for my NeRF.
  </p>

  <div class="grid grid-2">
    <figure>
      <img src="assets/imgs/drink_raw_01.jpg">
      <figcaption>Example raw training image.</figcaption>
    </figure>
    <figure>
      <img src="assets/imgs/drink_raw_02.jpg">
      <figcaption>Another view from the capture sequence.</figcaption>
    </figure>
  </div>

  <p class="muted">
    I estimated camera intrinsics/extrinsics using COLMAP, converted intrinsics to the NeRF
    coordinate system, and generated rays for each pixel exactly as in Part 2.1.
    This produced a complete multi-view dataset of my drink for NeRF training.
  </p>

  <h3>Training Setup</h3>
  <p>
    I train using batches of <strong>10,000 random rays</strong> per step, each with
    <strong>64 stratified samples</strong> along the ray. For each batch:
  </p>

  <ol>
    <li>Randomly sample rays across all cameras.</li>
    <li>Sample positions <code>t</code> along each ray.</li>
    <li>Query the NeRF MLP to get <code>(σᵢ, cᵢ)</code>.</li>
    <li>Apply my volume renderer (Part 2.5) to integrate along the ray.</li>
    <li>Compare with ground-truth pixel colors using MSE loss.</li>
  </ol>

  <pre><code>loss = F.mse_loss(pred_rgb, target_rgb)
optimizer.zero_grad()
loss.backward()
optimizer.step()</code></pre>

  <div class="grid grid-3">
    <figure>
      <img src="assets/imgs/drink_iter_000.png">
      <figcaption>Iteration 0 — network untrained.</figcaption>
    </figure>
    <figure>
      <img src="assets/imgs/drink_iter_200.png">
      <figcaption>Iteration 200 — basic silhouette forms.</figcaption>
    </figure>
    <figure>
      <img src="assets/imgs/drink_iter_1000.png">
      <figcaption>Iteration 1000 — cup geometry and texture appear.</figcaption>
    </figure>
  </div>

  <h3>PSNR on My Custom Validation Set</h3>
  <figure>
    <img src="assets/imgs/drink_psnr_curve.png">
    <figcaption>
      PSNR curve computed on six held-out camera poses from my drink dataset.
      The model steadily improves and reaches &gt; 23 dB.
    </figcaption>
  </figure>

  <h3>Novel Views + Spherical Rendering</h3>
  <p>
    After training, I rendered novel viewpoints of the cup by placing virtual cameras in a circular
    orbit around the object. This demonstrates that the NeRF learned a coherent 3D representation.
  </p>

  <div class="grid grid-3">
    <figure>
      <img src="assets/imgs/drink_novel_01.png">
      <figcaption>Novel view 1.</figcaption>
    </figure>
    <figure>
      <img src="assets/imgs/drink_novel_02.png">
      <figcaption>Novel view 2.</figcaption>
    </figure>
    <figure>
      <img src="assets/imgs/drink_spherical_frame.png">
      <figcaption>Frame from spherical rendering.</figcaption>
    </figure>
  </div>

  <p class="muted">
    The final video (drink_spherical.mp4) shows a smooth orbit around the cup, including consistent
    occlusion, shadows, and geometry. The network successfully reconstructs my custom scene.
  </p>

  <div class="hr"></div>
</section>

    
  </main>
</body>
</html>
